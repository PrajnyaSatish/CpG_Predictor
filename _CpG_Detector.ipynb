{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4T6QHHOnfcQ"
   },
   "source": [
    "# Part 1: Build CpG Detector\n",
    "\n",
    "Here we have a simple problem, given a DNA sequence (of N, A, C, G, T), count the number of CpGs in the sequence (consecutive CGs).\n",
    "\n",
    "We have defined a few helper functions / parameters for performing this task.\n",
    "\n",
    "We need you to build a LSTM model and train it to complish this task in PyTorch.\n",
    "\n",
    "A good solution will be a model that can be trained, with high confidence in correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mfS4cLmZD2oB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajnyasatish/anaconda3/envs/ml_projects/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "from functools import partial\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_f-brPAvKvTn"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "def set_seed(seed=13):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence(n_seqs: int, seq_len: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        yield [random.randint(0, 4) for _ in range(seq_len)]\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = { a: i for a, i in zip(alphabet, range(5))}\n",
    "int2dna = { i: a for a, i in zip(alphabet, range(5))}\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1651686469847,
     "user": {
      "displayName": "Ylex",
      "userId": "01820639168093643789"
     },
     "user_tz": 240
    },
    "id": "VK9Qg5GHYxOb",
    "outputId": "0a00bbb6-d9ac-4cf8-ed84-b55b335d7f51"
   },
   "outputs": [],
   "source": [
    "# we prepared two datasets for training and evaluation\n",
    "# training data scale we set to 2048\n",
    "# we test on 512\n",
    "\n",
    "def prepare_data(num_samples=100):\n",
    "    # prepared the training and test data\n",
    "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
    "    # step 1\n",
    "    X_dna_seqs_train = list(rand_sequence(num_samples))\n",
    "    \"\"\"\n",
    "    hint:\n",
    "        1. You can check X_dna_seqs_train by print, the data is ids which is your training X \n",
    "        2. You first convert ids back to DNA sequence\n",
    "        3. Then you run count_cpgs which will yield CGs counts - this will be the labels (Y)\n",
    "    \"\"\"\n",
    "    #step2\n",
    "    # use intseq_to_dnaseq here to convert ids back to DNA seqs\n",
    "    temp = [''.join(intseq_to_dnaseq(el)) for el in X_dna_seqs_train] \n",
    "    #step3\n",
    "    y_dna_seqs = [count_cpgs(el) for el in temp] \n",
    "    \n",
    "    return torch.tensor(X_dna_seqs_train), torch.tensor(y_dna_seqs).unsqueeze(1)\n",
    "    \n",
    "train_x, train_y = prepare_data(2048)\n",
    "test_x, test_y = prepare_data(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some config\n",
    "LSTM_HIDDEN = 64\n",
    "LSTM_LAYER = 25\n",
    "batch_size = 64\n",
    "learning_rate = 0.5\n",
    "dropout_p=0.5\n",
    "epoch_num = 100\n",
    "vocab_size = len(\"NACGT\")\n",
    "EMBED_DIM = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(list(zip(train_x[:200], train_y[:200])), batch_size, shuffle=True)\n",
    "test_Data_loader = torch.utils.data.DataLoader(list(zip(test_x, test_y)), batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q8fgxrM0LnLy"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class CpGPredictor(torch.nn.Module):\n",
    "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
    "    def __init__(self, num_layers, hidden_layer_dim, dropout, vocab_size, embed_dim):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_p = dropout\n",
    "        self.num_layers=num_layers\n",
    "        self.hidden_layer_dim = hidden_layer_dim\n",
    "        self.embed_dim=embed_dim\n",
    "        \n",
    "        ## Define the model layers\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = torch.nn.LSTM(embed_dim, hidden_size=hidden_layer_dim,\n",
    "                                  num_layers=num_layers, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_layer_dim, 1)\n",
    "        self.lstm2 = torch.nn.LSTM(hidden_layer_dim, hidden_size=hidden_layer_dim, batch_first=True)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout_p)\n",
    "        self.dropout2 = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        yhat, _ = self.lstm(embedded)\n",
    "        \n",
    "        ## Without Dropout, the model overfits too soon\n",
    "        yhat = self.dropout1(yhat)\n",
    "        yhat, _ = self.lstm2(yhat)\n",
    "#         yhat = self.dropout2(yhat)\n",
    "        out = self.linear(yhat[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model / loss function / optimizer etc.\n",
    "fixed_len_model = CpGPredictor(LSTM_LAYER, LSTM_HIDDEN, dropout_p, vocab_size, EMBED_DIM)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(fixed_len_model.parameters(), lr=learning_rate, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Loss =  34.10824251174927\n",
      "Epoch  1  Loss =  36.50367736816406\n",
      "Epoch  2  Loss =  18.9738187789917\n",
      "Epoch  3  Loss =  21.5706684589386\n",
      "Epoch  4  Loss =  17.118098258972168\n",
      "Epoch  5  Loss =  15.607149600982666\n",
      "Epoch  6  Loss =  11.690606474876404\n",
      "Epoch  7  Loss =  13.3591148853302\n",
      "Epoch  8  Loss =  9.2801034450531\n",
      "Epoch  9  Loss =  7.259066820144653\n",
      "Epoch  10  Loss =  8.05556058883667\n",
      "Epoch  11  Loss =  7.506715178489685\n",
      "Epoch  12  Loss =  6.672113656997681\n",
      "Epoch  13  Loss =  6.09421443939209\n",
      "Epoch  14  Loss =  6.944698929786682\n",
      "Epoch  15  Loss =  6.931966543197632\n",
      "Epoch  16  Loss =  7.463358163833618\n",
      "Epoch  17  Loss =  6.32133674621582\n",
      "Epoch  18  Loss =  6.496281027793884\n",
      "Epoch  19  Loss =  7.822375535964966\n",
      "Epoch  20  Loss =  7.36993134021759\n",
      "Epoch  21  Loss =  7.866607069969177\n",
      "Epoch  22  Loss =  7.457654356956482\n",
      "Epoch  23  Loss =  6.527387499809265\n",
      "Epoch  24  Loss =  6.640496611595154\n",
      "Epoch  25  Loss =  6.559722661972046\n",
      "Epoch  26  Loss =  8.420514464378357\n",
      "Epoch  27  Loss =  6.9159095287323\n",
      "Epoch  28  Loss =  6.091746091842651\n",
      "Epoch  29  Loss =  6.704357266426086\n",
      "Epoch  30  Loss =  6.954951167106628\n",
      "Epoch  31  Loss =  6.205938458442688\n",
      "Epoch  32  Loss =  6.819501876831055\n",
      "Epoch  33  Loss =  6.005607008934021\n",
      "Epoch  34  Loss =  6.242945432662964\n",
      "Epoch  35  Loss =  12.41930890083313\n",
      "Epoch  36  Loss =  11.511619091033936\n",
      "Epoch  37  Loss =  10.514131307601929\n",
      "Epoch  38  Loss =  8.510473847389221\n",
      "Epoch  39  Loss =  10.896406412124634\n",
      "Epoch  40  Loss =  9.080785155296326\n",
      "Epoch  41  Loss =  11.187811136245728\n",
      "Epoch  42  Loss =  12.018092393875122\n",
      "Epoch  43  Loss =  9.91470181941986\n",
      "Epoch  44  Loss =  7.188209652900696\n",
      "Epoch  45  Loss =  7.287984848022461\n",
      "Epoch  46  Loss =  8.943039655685425\n",
      "Epoch  47  Loss =  7.787647366523743\n",
      "Epoch  48  Loss =  10.708895325660706\n",
      "Epoch  49  Loss =  7.650114893913269\n",
      "Epoch  50  Loss =  9.031790971755981\n",
      "Epoch  51  Loss =  9.344162106513977\n",
      "Epoch  52  Loss =  7.129449725151062\n",
      "Epoch  53  Loss =  8.50961697101593\n",
      "Epoch  54  Loss =  8.05362093448639\n",
      "Epoch  55  Loss =  8.53983986377716\n",
      "Epoch  56  Loss =  8.602196216583252\n",
      "Epoch  57  Loss =  7.975858807563782\n",
      "Epoch  58  Loss =  10.865498065948486\n",
      "Epoch  59  Loss =  12.61500072479248\n",
      "Epoch  60  Loss =  7.5212562084198\n",
      "Epoch  61  Loss =  6.920742034912109\n",
      "Epoch  62  Loss =  7.276718378067017\n",
      "Epoch  63  Loss =  6.9335538148880005\n"
     ]
    }
   ],
   "source": [
    "# training (you can modify the code below)\n",
    "t_loss = .0\n",
    "loss_history = []\n",
    "train_rmse_plot = []\n",
    "test_rmse_plot = []\n",
    "train_acc_plot = []\n",
    "test_acc_plot = []\n",
    "min_loss = 100\n",
    "for epoch in range(epoch_num):\n",
    "    fixed_len_model.train()\n",
    "    fixed_len_model.zero_grad()\n",
    "    for X_batch, y_batch in train_data_loader:\n",
    "        y_pred = fixed_len_model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        t_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_history.append(t_loss)\n",
    "    t_loss = .0\n",
    "\n",
    "    fixed_len_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = fixed_len_model(train_x)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, train_y))\n",
    "        train_rmse_plot.append(train_rmse.item())\n",
    "        train_accuracy = (torch.sum(torch.round(y_pred).int() == train_y))/train_y.size()[0]\n",
    "        train_acc_plot.append(train_accuracy)\n",
    "        \n",
    "        y_pred = fixed_len_model(test_x)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, test_y))\n",
    "        test_rmse_plot.append(test_rmse.item())\n",
    "        test_accuracy = (torch.sum(torch.round(y_pred).int() == test_y))/test_y.size()[0]\n",
    "        test_acc_plot.append(test_accuracy)\n",
    "    print(\"Epoch \", epoch, \" Loss = \", loss_history[-1])\n",
    "    if loss_history[-1] < min_loss:\n",
    "        min_loss = loss_history[-1]\n",
    "        # Save the model\n",
    "        torch.save(fixed_len_model.state_dict(), 'models/fixed_len_model_weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot RMSE\n",
    "import itertools\n",
    "\n",
    "colors = ['r', 'g']\n",
    "cc = itertools.cycle(colors)\n",
    "plot_lines = []\n",
    "\n",
    "l1, = plt.plot(train_rmse_plot, '-', color='r')\n",
    "l2, = plt.plot(test_rmse_plot, '.-', color='g')\n",
    "\n",
    "plot_lines.append([l1, l2])\n",
    "\n",
    "legend1 = plt.legend(plot_lines[0], [\"Train RMSE plot\", \"Test RMSE plot\"], loc=1)\n",
    "plt.legend([l[0] for l in plot_lines], range(2), loc=4)\n",
    "plt.gca().add_artist(legend1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "\n",
    "colors = ['r', 'g']\n",
    "cc = itertools.cycle(colors)\n",
    "plot_lines = []\n",
    "\n",
    "l1, = plt.plot(train_acc_plot, '-', color='r')\n",
    "l2, = plt.plot(test_acc_plot, '--', color='g')\n",
    "\n",
    "plot_lines.append([l1, l2])\n",
    "\n",
    "legend1 = plt.legend(plot_lines[0], [\"Train accuracy\", \"Test accuracy\"], loc=1)\n",
    "plt.legend([l[0] for l in plot_lines], range(2), loc=4)\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.plot(train_acc_plot, c='r')\n",
    "plt.plot(test_acc_plot, c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_len_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = list(rand_sequence(1))\n",
    "print(\"\".join(intseq_to_dnaseq(example[0])))\n",
    "tester = torch.tensor(example)\n",
    "prediction = fixed_len_model(tester)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMrRf_aVDRJm"
   },
   "source": [
    "# Part 2: what if the DNA sequences are not the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint we will need following imports\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKvG-MNuXJr9"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "random.seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence_var_len(n_seqs: int, lb: int=16, ub: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        seq_len = random.randint(lb, ub)\n",
    "        yield [random.randint(1, 5) for _ in range(seq_len)]\n",
    "\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = {a: i for a, i in zip(alphabet, range(1, 6))}\n",
    "int2dna = {i: a for a, i in zip(alphabet, range(1, 6))}\n",
    "dna2int.update({\"pad\": 0})\n",
    "int2dna.update({0: \"<pad>\"})\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the task based on the change\n",
    "def prepare_data(num_samples=100, min_len=16, max_len=128):\n",
    "    # TODO prepared the training and test data\n",
    "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
    "    #step 1\n",
    "    X_dna_seqs_train = list(rand_sequence_var_len(num_samples, min_len, max_len))\n",
    "    #step 2\n",
    "    temp = [''.join(intseq_to_dnaseq(el)) for el in X_dna_seqs_train if el!=0]\n",
    "    #step3\n",
    "    y_dna_seqs = torch.tensor([count_cpgs(el) for el in temp]).unsqueeze(1)\n",
    "    \n",
    "    return X_dna_seqs_train, y_dna_seqs\n",
    "    \n",
    "    \n",
    "min_len, max_len = 64, 128\n",
    "train_x, train_y = prepare_data(2048, min_len, max_len)\n",
    "test_x, test_y = prepare_data(512, min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_pad(int_list, max_len=128, pad_var=0):\n",
    "    if (torch.is_tensor(int_list)):\n",
    "        length=int_list.size()[0]\n",
    "    else:\n",
    "        length = len(int_list)\n",
    "\n",
    "    pad_len = max_len-length\n",
    "    if pad_len > 0:\n",
    "        out = int_list+[0]*pad_len\n",
    "    else:\n",
    "        out= int_list\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, lists, labels) -> None:\n",
    "        self.lists = lists\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.lists[index]), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lists)\n",
    "\n",
    "    \n",
    "# this will be a collate_fn for dataloader to pad sequence  \n",
    "class PadSequence:\n",
    "    def __init__(self, padding_idx):\n",
    "        self.padding_idx = padding_idx\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        ##Padding\n",
    "        sequences = [custom_pad(el, 128, dna2int[\"pad\"]) for el,_ in batch]\n",
    "\n",
    "        labels = torch.tensor([y for _, y in batch]).unsqueeze(1)\n",
    "        return torch.tensor(sequences), labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(list(zip(train_x, train_y)), batch_size, shuffle=True, \n",
    "                                                collate_fn=PadSequence(dna2int[\"pad\"]))\n",
    "test_Data_loader = torch.utils.data.DataLoader(list(zip(test_x, test_y)), batch_size, shuffle=True, \n",
    "                                               collate_fn=PadSequence(dna2int[\"pad\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vocab_size = len(dna2int.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model / loss function / optimizer etc.\n",
    "padded_seq_model = CpGPredictor(LSTM_LAYER, LSTM_HIDDEN, dropout_p, padded_vocab_size, EMBED_DIM)\n",
    "loss_fn_2 = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(padded_seq_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "padded_seq_model.train()\n",
    "padded_seq_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_loss = .0\n",
    "loss_history = []\n",
    "min_loss = 100\n",
    "train_rmse_plot = []\n",
    "test_rmse_plot = []\n",
    "train_accuracy_plot = []\n",
    "test_accuracy_plot = []\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for X_batch, y_batch in train_data_loader:\n",
    "        y_pred = padded_seq_model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        t_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_history.append(t_loss)\n",
    "    print(t_loss)\n",
    "    t_loss = .0\n",
    "    \n",
    "    ## Evaluate\n",
    "    with torch.no_grad():\n",
    "        y_pred = padded_seq_model(train_x)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, train_y))\n",
    "        train_rmse_plot.append(train_rmse.item())\n",
    "        train_accuracy = (torch.sum(torch.round(y_pred).int() == train_y))/train_y.size()[0]\n",
    "\n",
    "        y_pred = padded_seq_model(test_x)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, test_y))\n",
    "        test_rmse_plot.append(test_rmse.item())\n",
    "        test_accuracy = (torch.sum(torch.round(y_pred).int() == test_y))/test_y.size()[0]\n",
    "    if min_loss > loss_history[-1]:\n",
    "        torch.save(padded_seq_model.state_dict(), \"models/padded_seq_model_weights.pth\")\n",
    "        min_loss = loss_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_seq_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = list(rand_sequence(1))\n",
    "print(\" \".join(intseq_to_dnaseq(example[0])))\n",
    "tester = torch.tensor(example)\n",
    "print(tester)\n",
    "prediction = padded_seq_model(tester)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "\n",
    "colors = ['r', 'g']\n",
    "cc = itertools.cycle(colors)\n",
    "plot_lines = []\n",
    "\n",
    "l1, = plt.plot(train_acc_plot, '-', color='r')\n",
    "l2, = plt.plot(test_acc_plot, '--', color='g')\n",
    "\n",
    "plot_lines.append([l1, l2])\n",
    "\n",
    "legend1 = plt.legend(plot_lines[0], [\"Train accuracy\", \"Test accuracy\"], loc=1)\n",
    "plt.legend([l[0] for l in plot_lines], range(2), loc=4)\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.plot(train_acc_plot, c='r')\n",
    "plt.plot(test_acc_plot, c='g')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Xi Yangs Copy of broken-nn-template.ipynb",
   "provenance": [
    {
     "file_id": "13GlbI_pdKNES8I718iwl1KNnMZ73iOOn",
     "timestamp": 1651680757732
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
